{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from pygments.lexers import JavaLexer\n",
    "from pygments import lex\n",
    "from pygments.token import Token\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tokens(code):\n",
    "    \"\"\"\n",
    "    Extracts all tokens from the given Java code using Pygments.\n",
    "    \n",
    "    Args:\n",
    "        code (str): Java code as a string.\n",
    "        \n",
    "    Returns:\n",
    "        str: All tokens extracted from the code, joined by spaces.\n",
    "    \"\"\"\n",
    "    lexer = JavaLexer()\n",
    "    tokens = []\n",
    "    for ttype, value in lex(code, lexer):\n",
    "        # Excluimos solo espacios en blanco para reducir ruido\n",
    "        if not str(ttype).startswith('Token.Text.Whitespace'):\n",
    "            val = value.strip()\n",
    "            if val:\n",
    "                # IMPORTANTE: Usar exactamente el mismo formato que en el entrenamiento\n",
    "                # En el entrenamiento usabas f\"{ttype}:{val}\"\n",
    "                tokens.append(f\"{ttype}:{val}\")\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_similarity(file1, file2, model_path='rf_model.pkl', vectorizer_path='rf_model_vectorizer.pkl'):\n",
    "    \"\"\"\n",
    "    Predice la similitud entre dos archivos de código Java.\n",
    "\n",
    "    Args:\n",
    "        file1 (str): Ruta al primer archivo Java.\n",
    "        file2 (str): Ruta al segundo archivo Java.\n",
    "        model_path (str): Ruta al modelo entrenado.\n",
    "        vectorizer_path (str): Ruta al vectorizador entrenado.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (score, is_similar)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Cargar modelo y vectorizador\n",
    "        model = joblib.load(model_path)\n",
    "        vectorizer = joblib.load(vectorizer_path)\n",
    "        \n",
    "        # Leer archivos con manejo de errores\n",
    "        try:\n",
    "            with open(file1, 'r', encoding='utf-8') as f:\n",
    "                code1 = f.read()\n",
    "        except UnicodeDecodeError:\n",
    "            # Intentar con otra codificación si UTF-8 falla\n",
    "            with open(file1, 'r', encoding='latin-1') as f:\n",
    "                code1 = f.read()\n",
    "        \n",
    "        try:\n",
    "            with open(file2, 'r', encoding='utf-8') as f:\n",
    "                code2 = f.read()\n",
    "        except UnicodeDecodeError:\n",
    "            with open(file2, 'r', encoding='latin-1') as f:\n",
    "                code2 = f.read()\n",
    "        \n",
    "        # Regla especial para archivos vacíos o muy pequeños\n",
    "        if len(code1.strip()) < 10 or len(code2.strip()) < 10:\n",
    "            print(\"Al menos uno de los archivos está vacío o es muy pequeño.\")\n",
    "            return 0.0, False\n",
    "        \n",
    "        # Tokenizar el código completo\n",
    "        t1 = extract_tokens(code1)\n",
    "        t2 = extract_tokens(code2)\n",
    "        token_pair = f\"{t1} {t2}\"\n",
    "        \n",
    "        # Vectorizar usando el mismo vectorizador del entrenamiento\n",
    "        X = vectorizer.transform([token_pair])\n",
    "        \n",
    "        # Para modelos como MLP que no tienen transform, convertimos a array\n",
    "        X_features = X.toarray()\n",
    "        \n",
    "        # Predecir\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            # Para modelos que pueden dar probabilidades\n",
    "            proba = model.predict_proba(X_features)[0]\n",
    "            similarity_score = proba[1] if len(proba) > 1 else proba[0]\n",
    "        else:\n",
    "            # Para modelos que solo dan la clase\n",
    "            prediction = model.predict(X_features)[0]\n",
    "            similarity_score = float(prediction)\n",
    "        \n",
    "        # Umbral personalizable\n",
    "        threshold = 0.50\n",
    "        is_similar = similarity_score >= threshold\n",
    "        \n",
    "        return similarity_score, is_similar\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error en la predicción: {str(e)}\")\n",
    "        return None, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RESULTADOS DE LA DETECCIÓN DE PLAGIO ===\n",
      "Archivo 1: original.java\n",
      "Archivo 2: plagiarized.java\n",
      "Puntaje de similitud: 0.9999 (0-1)\n",
      "Plagio detectado: SÍ\n",
      "==========================================\n",
      "\n",
      "INFORMACIÓN ADICIONAL:\n",
      "- Tamaño del archivo 1: 128 bytes\n",
      "- Tamaño del archivo 2: 128 bytes\n",
      "- Modelo utilizado: mlp_model.pkl\n",
      "\n",
      "RESULTADO_JSON: {'score': 0.9999446264537352, 'is_plagiarism': True}\n"
     ]
    }
   ],
   "source": [
    "   \n",
    "file1 = \"original.java\"\n",
    "file2 = \"plagiarized.java\"\n",
    "    \n",
    "model_path = \"mlp_model.pkl\"\n",
    "vectorizer_path = \"mlp_model_vectorizer.pkl\"\n",
    "    \n",
    "# Verificar que los archivos existen\n",
    "for f in [file1, file2, model_path, vectorizer_path]:\n",
    "    if not os.path.exists(f):\n",
    "        print(f\"Error: El archivo {f} no existe.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "score, is_similar = predict_similarity(file1, file2, model_path, vectorizer_path)\n",
    "    \n",
    "if score is not None:\n",
    "        print(\"\\n=== RESULTADOS DE LA DETECCIÓN DE PLAGIO ===\")\n",
    "        print(f\"Archivo 1: {file1}\")\n",
    "        print(f\"Archivo 2: {file2}\")\n",
    "        print(f\"Puntaje de similitud: {score:.4f} (0-1)\")\n",
    "        print(f\"Plagio detectado: {'SÍ' if is_similar else 'NO'}\")\n",
    "        print(\"==========================================\\n\")\n",
    "        \n",
    "        # Información adicional para debug\n",
    "        print(\"INFORMACIÓN ADICIONAL:\")\n",
    "        print(f\"- Tamaño del archivo 1: {os.path.getsize(file1)} bytes\")\n",
    "        print(f\"- Tamaño del archivo 2: {os.path.getsize(file2)} bytes\")\n",
    "        print(f\"- Modelo utilizado: {os.path.basename(model_path)}\")\n",
    "        \n",
    "        # Salida con formato para procesamiento automático (si se necesita)\n",
    "        print(f\"\\nRESULTADO_JSON: {{'score': {score}, 'is_plagiarism': {is_similar}}}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
