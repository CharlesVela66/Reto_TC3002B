{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9eb6c7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from pygments import lex\n",
    "from pygments.lexers import JavaLexer\n",
    "from pygments.token import Token\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20022a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = Path().cwd().parent\n",
    "BASE_PATH = ROOT / \"dataset\" / \"versions\" / \"bplag_version_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ed95e47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_java_files(base_path):\n",
    "    \"\"\"\n",
    "    Recursively reads all .java files from the given base path.\n",
    "    \n",
    "    Args:\n",
    "        base_path (str): Path to the base directory containing submission pairs.\n",
    "    \n",
    "    Returns:\n",
    "        data (list): List of tuples (submission_id, code_content).\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    # Iterate over all submission pairs\n",
    "    for submission_pair in os.listdir(base_path):\n",
    "        pair_path = os.path.join(base_path, submission_pair)\n",
    "        \n",
    "        if os.path.isdir(pair_path):\n",
    "            # Iterate over each submission inside the pair\n",
    "            for submission_id in os.listdir(pair_path):\n",
    "                submission_path = os.path.join(pair_path, submission_id)\n",
    "                \n",
    "                if os.path.isdir(submission_path):\n",
    "                    # Look for .java files inside the submission directory\n",
    "                    for file in os.listdir(submission_path):\n",
    "                        if file.endswith('.java'):\n",
    "                            file_path = os.path.join(submission_path, file)\n",
    "                            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                                code = f.read()\n",
    "                                data.append((submission_id, code))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bbcf8253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total submissions loaded: 1822\n",
      "\n",
      "First 2 submissions loaded:\n",
      "Submission ID: 0017d438\n",
      "Code snippet:\n",
      "import java.io.BufferedReader;\n",
      "import java.io.DataInputStream;\n",
      "import java.io.IOException;\n",
      "import java.io.InputStreamReader;\n",
      "import java.util.*;\n",
      "public class Main {\n",
      "    static int modulo=998244353;\n",
      "    public static void main(String[] args) {\n",
      "       \n",
      "        FastScanner in = new FastScanner();\n",
      "     ...\n",
      "\n",
      "Submission ID: 9852706b\n",
      "Code snippet:\n",
      "import java.io.BufferedReader;\n",
      "import java.io.IOException;\n",
      "import java.io.InputStreamReader;\n",
      "import java.io.PrintWriter;\n",
      "import java.util.*;\n",
      "\n",
      "public class A {\n",
      "    static List<Integer> [] adj;\n",
      "    static ArrayList<Integer> temp;\n",
      "    static int mod = (int) 1e9+7;\n",
      "    static boolean[] vis = new boolean...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the Java files\n",
    "java_files_data = read_java_files(BASE_PATH)\n",
    "\n",
    "# Print the number of submissions loaded and the first few entries\n",
    "print(f\"Total submissions loaded: {len(java_files_data)}\")\n",
    "print(\"\\nFirst 2 submissions loaded:\")\n",
    "for submission_id, code in java_files_data[:2]:\n",
    "    print(f\"Submission ID: {submission_id}\\nCode snippet:\\n{code[:300]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26774854",
   "metadata": {},
   "source": [
    "## Extracción de tokens\n",
    "\n",
    "Para esta sección se utiliza la librería Pygments como analizador léxico. Esta librería permite extraer los tokens de un código fuente y clasificarlos en diferentes categorías. En este caso, se utilizará para extraer los tokens de código fuente en Java."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b981f462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tokens(code):\n",
    "    \"\"\"\n",
    "    Extracts tokens from the given Java code using Pygments.\n",
    "    \n",
    "    Args:\n",
    "        code (str): Java code as a string.\n",
    "        \n",
    "    Returns:\n",
    "        tokens (list): List of tokens extracted from the code.\n",
    "    \"\"\"\n",
    "    lexer = JavaLexer()\n",
    "    tokens = []\n",
    "    for ttype, value in lex(code, lexer):\n",
    "        if ttype in Token.Name or ttype in Token.Keyword or ttype in Token.Operator:\n",
    "            val = value.strip()\n",
    "            if val:\n",
    "                tokens.append(f\"{ttype.__class__.__name__}:{val}\")\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "58bd1fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_pairs = []\n",
    "labels = []\n",
    "\n",
    "for i in range(0, len(java_files_data), 2):\n",
    "    try:\n",
    "        id1, code1 = java_files_data[i]\n",
    "        id2, code2 = java_files_data[i+1]\n",
    "    except IndexError:\n",
    "        break\n",
    "    t1 = extract_tokens(code1)\n",
    "    t2 = extract_tokens(code2)\n",
    "    token_pairs.append(f\"{t1} {t2}\")\n",
    "    labels.append(1 if id1 == id2 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5025b334",
   "metadata": {},
   "source": [
    "## Vectorización de tokens\n",
    "\n",
    "Para la vectorización de los tokens, se utilizará la librería Scikit-learn. Esta librería permite transformar los tokens extraídos en vectores numéricos que pueden ser utilizados como entrada para el modelo. En este caso, se utilizará el método `TfidfVectorizer` para transformar los tokens en vectores numéricos. Este método asigna un peso a cada token en función de su frecuencia en el documento y su frecuencia en el corpus. Esto permite que los tokens más relevantes tengan un mayor peso en el vector resultante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e75ae232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(token_pairs):\n",
    "    \"\"\"\n",
    "    Vectorizes the given token pairs using TF-IDF.\n",
    "    \n",
    "    Args:\n",
    "        token_pairs (list): List of tokens to be vectorized.\n",
    "    \n",
    "    Returns:\n",
    "        vectorizer (TfidfVectorizer): Fitted TF-IDF vectorizer.\n",
    "        X (sparse matrix): TF-IDF matrix of the token pairs.\n",
    "        Y (list): Labels corresponding to the token pairs.\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(token_pairs)\n",
    "    Y = labels\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "00082e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = vectorize(token_pairs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
